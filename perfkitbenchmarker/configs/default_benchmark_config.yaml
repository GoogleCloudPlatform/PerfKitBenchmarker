# PKB benchmark configs that point to a benchmark module via 'name: <BENCHMARK_NAME>'.
# This is meant to provide commonly used configs that can not be expressed through
# flag defaults or default benchmark configs.
default_disk_spec: &default_disk_spec
  GCP:
    disk_size: 200
    disk_type: pd-balanced
  AWS:
    disk_size: 200
    disk_type: gp3
  Azure:
    disk_size: 200
    disk_type: PremiumV2_LRS

dpdk_pktgen_small_pkt_loss:
  name: dpdk_pktgen
  flags:
    dpdk_pktgen_tx_burst: 128
    dpdk_pktgen_rx_burst: 128
    dpdk_pktgen_packet_loss_threshold_rates: 0.0001
    dpdk_pktgen_txd: 2048
    dpdk_pktgen_rxd: 2048
    dpdk_pktgen_mbuf_cache_size: 128
    dpdk_pktgen_mbufs_per_port_multiplier: 8

speccpu2017_int_rate_GCC_O3_x86:
  name: speccpu2017
  flags:
    runspec_config: pkb-gcc-linux-x86.cfg
    # Passing default explicitly for metadata
    spec17_gcc_flags: -O3 -march=native
  vm_groups:
    default:
      disk_spec: *default_disk_spec

speccpu2017_int_rate_GCC_OFAST_x86:
  name: speccpu2017
  flags:
    runspec_config: pkb-gcc-linux-x86.cfg
    spec17_gcc_flags: -Ofast -funroll-loops -flto -ljemalloc -z muldefs -fallow-argument-mismatch -march=native
  vm_groups:
    default:
      disk_spec: *default_disk_spec

speccpu2017_int_rate_GCC_O3_arm:
  name: speccpu2017
  flags:
    runspec_config: pkb-gcc-linux-aarch64-1.1.0.cfg
    spec17_gcc_flags: -O3 -mcpu=native
  vm_groups:
    default:
      disk_spec: *default_disk_spec

speccpu2017_int_rate_GCC_OFAST_arm:
  name: speccpu2017
  flags:
    runspec_config: pkb-gcc-linux-aarch64-1.1.0.cfg
    spec17_gcc_flags: -Ofast -funroll-loops -flto -ljemalloc -z muldefs -fallow-argument-mismatch -march=native
  vm_groups:
    default:
      disk_spec: *default_disk_spec

netperf_tcp_rr:
  name: netperf
  flags:
    netperf_benchmarks: TCP_RR
    sar: True

netperf_tcp_stream:
  name: netperf
  flags:
    netperf_benchmarks: TCP_STREAM
    netperf_num_streams: 1,8,16,200
    placement_group_style: none
    sar: True

mysql_tpcc:
  name: hammerdbcli
  flags:
    db_engine: mysql
    db_engine_version: '8.0'
    hammerdbcli_optimized_server_configuration: restorable
    hammerdbcli_tpcc_all_warehouse: True
    hammerdbcli_tpcc_time_profile: True
    use_managed_db: False
    db_high_availability: False
    iostat: True
    innodb_buffer_pool_ratio: 0.8
    timeout_minutes: 360
  relational_db:
    engine: mysql
    vm_groups:
      clients:
        disk_spec: *default_disk_spec
      servers:
        vm_spec:
          # TODO(andytzhu) - Add AWS/Azure
          GCP:
            boot_disk_size: 200
        disk_spec:
          # 4TB pd-balanced has 27k IOPS, 1260 MB/s throughput.
          GCP:
            disk_size: 4000
            disk_type: pd-balanced
          # Aim for 40k iops, 1200 MB/s throughput, at least 2 TB capacity, not costing more than $400/month.
          AWS:
            disk_size: 500
            disk_type: gp3
            provisioned_iops: 10000
            provisioned_throughput: 300
            num_striped_disks: 4
          Azure:
            disk_size: 2000
            disk_type: PremiumV2_LRS
            provisioned_iops: 40000
            provisioned_throughput: 1200

postgres_tpcc:
  name: hammerdbcli
  flags:
    db_engine: postgres
    db_engine_version: '13'
    hammerdbcli_version: '4.3'
    hammerdbcli_optimized_server_configuration: restorable
    hammerdbcli_tpcc_all_warehouse: True
    hammerdbcli_tpcc_time_profile: True
    hammerdbcli_tpcc_log_transactions: True
    hammerdbcli_tpcc_duration: 30
    hammerdbcli_build_timeout: 19000
    use_managed_db: False
    db_high_availability: False
    iostat: True
    postgres_shared_buffer_ratio: 0.25
    timeout_minutes: 360
  relational_db:
    engine: postgres
    vm_groups:
      clients:
        disk_spec: *default_disk_spec
      servers:
        vm_spec:
          # TODO(andytzhu) - Add AWS/Azure
          GCP:
            boot_disk_size: 200
        disk_spec:
          # 4TB pd-ssd has 27k IOPS, 1260 MB/s throughput.
          GCP:
            disk_size: 4000
            disk_type: pd-balanced
          # These aim to match a 4 TB PD-SSD in performance and price. AWS can get better performance.
          AWS:
            disk_size: 1000
            disk_type: gp3
            provisioned_iops: 16000
            provisioned_throughput: 300
            num_striped_disks: 4
          Azure:
            disk_size: 2000
            disk_type: PremiumV2_LRS
            provisioned_iops: 64000
            provisioned_throughput: 1200

hpcc_stream:
  name: hpcc
  flags:
    enable_transparent_hugepages: True
    hpcc_benchmarks: StarSTREAM
    hpcc_math_library: openblas
    hpcc_numa_binding: True
    num_vms: 1

multiload:
  name: multichase
  flags:
    multichase_benchmarks: multiload

# https://github.com/elastic/rally-tracks/tree/master/elastic/logs
elasticsearch_rally_logging_single_node:
  name: esrally
  vm_groups:
    servers:
      vm_count: 1
  flags:
    # Increase dataset size, use the default of 8 bulk indexing clients
    # as increasing further does not improve performance and may lead to
    # connection timeout errors
    esrally_track_params: 'number_of_replicas:0,raw_data_volume_per_day:1.0GB'

# TODO: memtier_key_maximum, memtier_load_key_maximum
redis_memtier_full_house_loaded:
  name: redis_memtier
  flags:
    memtier_clients: 8
    memtier_requests: -1
    memtier_run_duration: 300
    memtier_threads: 16
    redis_eviction_policy: allkeys-lru
    redis_server_version: '5.0.14'
    redis_total_num_processes: 0

redis_memtier_full_house_loaded:
  name: redis_memtier
  flags:
    memtier_clients: 8
    memtier_requests: -1
    memtier_run_duration: 300
    memtier_threads: 16
    redis_eviction_policy: allkeys-lru
    redis_server_version: '5.0.14'
    redis_total_num_processes: 0

# memtier_key_maximum and redis_server_io_threads must be specified.
redis_memtier_single_base: &redis_memtier_single_base
  redis_server_io_threads_do_reads: True
  memtier_clients: 12
  memtier_threads: 32
  memtier_requests: -1
  memtier_run_duration: 300
  redis_server_version: '7.2.6'
  redis_total_num_processes: 1
  enable_transparent_hugepages: False  # never
  redis_eviction_policy: allkeys-lru
  sar: True

redis_memtier_caching_single_node:
  name: redis_memtier
  flags:
    <<: *redis_memtier_single_base
    memtier_ratio: '1:4'
    redis_server_io_threads: '1,2,3,4'

redis_memtier_caching_cluster:
  name: redis_memtier
  flags:
    <<: *redis_memtier_single_base
    memtier_ratio: '1:4'
    redis_server_cluster_mode: True
    memtier_cluster_mode: True
  vm_groups:
    servers:
      vm_count: 3

redis_memtier_session_storage_disk_spec: &redis_memtier_session_storage_disk_spec
  disk_spec:
    GCP:
      disk_size: 500
      disk_type: hyperdisk-balanced
      provisioned_iops: 12000
      provisioned_throughput: 1000
    AWS:
      disk_size: 500
      disk_type: gp3
      provisioned_iops: 12000
      provisioned_throughput: 1000
    Azure:
      disk_size: 500
      disk_type: PremiumV2_LRS
      provisioned_iops: 12000
      provisioned_throughput: 1000

redis_memtier_session_storage_single_node:
  name: redis_memtier
  flags:
    <<: *redis_memtier_single_base
    memtier_ratio: '1:1'
    redis_aof: True
  vm_groups:
    servers:
      <<: *redis_memtier_session_storage_disk_spec

redis_memtier_session_storage_cluster:
  name: redis_memtier
  flags:
    <<: *redis_memtier_single_base
    memtier_ratio: '1:1'
    redis_aof: True
    redis_server_cluster_mode: True
    memtier_cluster_mode: True
  vm_groups:
    servers:
      vm_count: 3
      <<: *redis_memtier_session_storage_disk_spec


sparksql_tpcds_1t:
  name: dpb_sparksql_benchmark
  flags:
    create_and_boot_post_task_delay: 2
    dpb_sparksql_copy_to_hdfs: True
    dpb_sparksql_create_hive_tables: False
    dpb_sparksql_data_compression: snappy
    dpb_sparksql_data_format: parquet
    dpb_sparksql_order: 1,2,3,4,5,6,7,8,9,10,11,12,13,14a,14b,15,16,17,18,19,20,21,22,23a,23b,24a,24b,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39a,39b,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99
    dpb_sparksql_query: tpcds_2_4
  dpb_service:
    service_type: unmanaged_spark_cluster
    worker_count: 8
    worker_group:
      vm_spec:
        GCP:
          num_local_ssds: 0
      disk_spec:
        # Targeting 480MB/s bandwidth.
        GCP:
          disk_size: 1214
          disk_type: pd-balanced
        AWS:
          disk_size: 500
          disk_type: gp3
          provisioned_iops: 5000
          provisioned_throughput: 480

sparksql_tpcds_1t_tuned:
  name: dpb_sparksql_benchmark
  flags:
    create_and_boot_post_task_delay: 2
    dpb_export_job_stats: True
    dpb_sparksql_copy_to_hdfs: True
    dpb_sparksql_create_hive_tables: False
    dpb_sparksql_data_compression: snappy
    dpb_sparksql_data_format: parquet
    dpb_sparksql_order: 1,2,3,4,5,6,7,8,9,10,11,12,13,14a,14b,15,16,17,18,19,20,21,22,23a,23b,24a,24b,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39a,39b,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99
    dpb_sparksql_query: tpcds_2_4
    hadoop_hdfs_client_readahead: 0
    hadoop_hdfs_replication: 1
    hadoop_version: 3.4.1
    openjdk_version: 11
    spark_version: 3.5.6
    sparksql_readahead_kb: 1024
  dpb_service:
    service_type: unmanaged_spark_cluster
    worker_count: 8
    worker_group:
      vm_spec:
        GCP:
          num_local_ssds: 0
      disk_count: 3
      disk_spec:
        GCP:
          disk_size: 320
          disk_type: hyperdisk-balanced
          provisioned_iops: 160000
          provisioned_throughput: 2400
        AWS:
          disk_size: 500
          disk_type: gp3
          provisioned_iops: 5000
          provisioned_throughput: 480

sysbench_oltp_base: &sysbench_oltp_base
  sysbench_table_size: 50000000
  sysbench_tables: 8
  sar: True

sysbench_oltp_base_lssd: &sysbench_oltp_base_lssd
  innodb_buffer_pool_size: 8
  <<: *sysbench_oltp_base

mysql_sysbench_oltp_read_write:
  name: unmanaged_mysql_sysbench
  flags:
    sysbench_testname: oltp_read_write
    <<: *sysbench_oltp_base

mysql_sysbench_oltp_read_only:
  name: unmanaged_mysql_sysbench
  flags:
    sysbench_testname: oltp_read_only
    <<: *sysbench_oltp_base

mysql_sysbench_oltp_write_only:
  name: unmanaged_mysql_sysbench
  flags:
    sysbench_testname: oltp_write_only
    <<: *sysbench_oltp_base

mysql_sysbench_oltp_read_only_lssd:
  name: unmanaged_mysql_sysbench
  flags:
    sysbench_testname: oltp_read_only
    <<: *sysbench_oltp_base_lssd

mysql_sysbench_oltp_read_write_lssd:
  name: unmanaged_mysql_sysbench
  flags:
    sysbench_testname: oltp_read_write
    <<: *sysbench_oltp_base_lssd

mysql_sysbench_oltp_write_only_lssd:
  name: unmanaged_mysql_sysbench
  flags:
    sysbench_testname: oltp_write_only
    <<: *sysbench_oltp_base_lssd

mysql_sysbench_tpcc:
  name: unmanaged_mysql_sysbench
  flags:
    sar: True
    sysbench_testname: percona_tpcc
    sysbench_tables: 10
    sysbench_use_fk: False
    sysbench_txn_isolation_level: RC

postgresql_sysbench_oltp_base: &postgresql_sysbench_oltp_base
  sysbench_table_size: 21000000
  sysbench_tables: 8
  postgresql_shared_buffer_size: 42G
  sar: True

postgres_sysbench_oltp_read_write:
  name: unmanaged_postgresql_sysbench
  flags:
    sysbench_testname: oltp_read_write
    <<: *postgresql_sysbench_oltp_base

postgres_sysbench_oltp_read_only:
  name: unmanaged_postgresql_sysbench
  flags:
    sysbench_testname: oltp_read_only
    <<: *postgresql_sysbench_oltp_base

postgres_sysbench_oltp_write_only:
  name: unmanaged_postgresql_sysbench
  flags:
    sysbench_testname: oltp_write_only
    <<: *postgresql_sysbench_oltp_base

postgres_sysbench_tpcc:
  name: unmanaged_postgresql_sysbench
  flags:
    sysbench_testname: percona_tpcc
    sysbench_tables: 10
    sysbench_scale: 100
    sysbench_use_fk: False

fio_latency:
  name: fio
  flags:
    data_disk_size: 1024
    provisioned_iops: 16000
    provisioned_throughput: 1000
    fio_generate_scenarios: rand_8k_read_100%,rand_8k_write_100%
    fio_io_depths: 1
    fio_num_jobs: 1
    fio_runtime: 300
    fio_target_mode: against_device_with_fill

fio_object_storage_latency_8kb:
  name: fio_object_storage
  flags:
    fio_generate_scenarios: rand_8k_read_800KB_iodepth-1_numjobs-1
    fio_fill_size: 800K
    fio_fill_block_size: 8k
    fio_nr_files: 100
    fio_ramptime: 0
    fio_runtime: 0

gpu_jobs_provisioning:
  name: jobs_benchmark
  base_job:
     job_type: 'GoogleCloudRunJob'
     job_region: 'us-central1'
     job_backend: '16Gi'
     job_count: 1
     job_gpu_count: 1
     image_directory: 'serverless/echo_job'

hadoop_dfsio:
  name: dpb_testdfsio_benchmark
  flags:
    dfsio_delay_read_sec: 1800
    dfsio_file_sizes_list: [49152]
    dfsio_fs: hdfs
    dfsio_num_files_list: [15]
    dfsio_readahead_kb: 1024
    dpb_export_job_stats: True
    hadoop_hdfs_client_readahead: 0
    hadoop_hdfs_replication: 1
    hadoop_map_slots_per_core: 1
    hadoop_reduce_slots_per_core: 1
    hadoop_version: 3.4.1
    num_cpus_override: 16
    openjdk_version: 11
    yarn_scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
  dpb_service:
    service_type: unmanaged_dpb_svc_yarn_cluster
    worker_count: 1
    worker_group:
      vm_spec:
        GCP:
          num_local_ssds: 0
      disk_count: 3
      disk_spec:
        GCP:
          disk_size: 9200
          disk_type: hyperdisk-throughput
          provisioned_throughput: 800

diskspd_read_only:
  name: diskspd
  flags: &diskspd_read_only_flags
    diskspd_prefill_duration: 2700
    diskspd_write_read_ratio: 0
    after_prepare_sleep_time: 300 # Wait after the prepare phase for prefill to complete.

diskspd_write_only:
  name: diskspd
  flags: &diskspd_write_only_flags
    diskspd_write_read_ratio: 100

diskspd_single_thread_latency:
  name: diskspd
  flags: &diskspd_single_thread_latency_flags
    diskspd_thread_number_per_file: 1
    diskspd_outstanding_io: 1
    diskspd_block_size: 4k
    diskspd_access_pattern: r

diskspd_max_iops:
  name: diskspd
  flags: &diskspd_max_iops_flags
    diskspd_thread_number_per_file: 4,8,16,32,64,96,128
    diskspd_outstanding_io: 64
    diskspd_block_size: 4k
    diskspd_access_pattern: r

diskspd_max_throughput:
  name: diskspd
  flags: &diskspd_max_throughput_flags
    diskspd_thread_number_per_file: 8,16
    diskspd_outstanding_io: 8,16,64
    diskspd_block_size:  1M
    diskspd_access_pattern: s

diskspd_single_thread_latency_read_only:
  name: diskspd
  flags:
    <<: *diskspd_read_only_flags
    <<: *diskspd_single_thread_latency_flags

diskspd_single_thread_latency_write_only:
  name: diskspd
  flags:
    <<: *diskspd_write_only_flags
    <<: *diskspd_single_thread_latency_flags

diskspd_max_iops_read_only:
  name: diskspd
  flags:
    <<: *diskspd_read_only_flags
    <<: *diskspd_max_iops_flags

diskspd_max_iops_write_only:
  name: diskspd
  flags:
    <<: *diskspd_write_only_flags
    <<: *diskspd_max_iops_flags

diskspd_max_throughput_read_only:
  name: diskspd
  flags:
    <<: *diskspd_read_only_flags
    <<: *diskspd_max_throughput_flags

diskspd_max_throughput_write_only:
  name: diskspd
  flags:
    <<: *diskspd_write_only_flags
    <<: *diskspd_max_throughput_flags
