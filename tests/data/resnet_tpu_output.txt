Warning: Permanently added '35.188.219.76' (ECDSA) to the list of known hosts.
W0725 16:57:24.014842 139816308807424 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/__init__.py", line 41, in autodetect
    from . import file_cache
  File "/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/file_cache.py", line 41, in <module>
    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')
ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth
W0725 16:57:24.252393 139816308807424 tf_logging.py:125] Estimator's model_fn (<function resnet_model_fn at 0x7f29620cee60>) includes params argument, but params are not passed to Estimator.
I0725 16:57:24.253319 139816308807424 tf_logging.py:115] Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
cluster_def {
  job {
    name: "worker"
    tasks {
      value: "10.240.0.2:8470"
    }
  }
}
, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2961a313d0>, '_model_dir': 'gs://fd3add40', '_save_checkpoints_steps': 1251, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tpu_config': TPUConfig(iterations_per_loop=1251, num_shards=8, computation_shape=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None), '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f29620d8850>, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_evaluation_master': u'grpc://10.240.0.2:8470', '_global_id_in_cluster': 0, '_master': u'grpc://10.240.0.2:8470'}
I0725 16:57:24.253607 139816308807424 tf_logging.py:115] _TPUContext: eval_on_tpu True
I0725 16:57:24.253820 139816308807424 tf_logging.py:115] Precision: bfloat16
I0725 16:57:24.710988 139816308807424 tf_logging.py:115] Training for 112590 steps (89.99 epochs in total). Current step 0.
I0725 16:57:25.049052 139816308807424 tf_logging.py:115] Querying Tensorflow master (grpc://10.240.0.2:8470) for TPU system metadata.
2018-07-25 16:57:25.061519: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.
I0725 16:57:25.167747 139816308807424 tf_logging.py:115] Found TPU system:
I0725 16:57:25.168109 139816308807424 tf_logging.py:115] *** Num TPU Cores: 8
I0725 16:57:25.168456 139816308807424 tf_logging.py:115] *** Num TPU Workers: 1
I0725 16:57:25.168515 139816308807424 tf_logging.py:115] *** Num TPU Cores Per Worker: 8
I0725 16:57:25.168567 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1)
I0725 16:57:25.168777 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184)
I0725 16:57:25.168842 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184)
I0725 16:57:25.168889 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184)
I0725 16:57:25.168936 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184)
I0725 16:57:25.168982 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184)
I0725 16:57:25.169027 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184)
I0725 16:57:25.169070 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184)
I0725 16:57:25.169131 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184)
I0725 16:57:25.169190 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184)
I0725 16:57:25.169260 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184)
I0725 16:57:25.169317 139816308807424 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184)
I0725 16:57:25.192455 139816308807424 tf_logging.py:115] Calling model_fn.
I0725 16:57:33.948093 139816308807424 tf_logging.py:115] Create CheckpointSaverHook.
I0725 16:57:34.162679 139816308807424 tf_logging.py:115] Done calling model_fn.
I0725 16:57:37.531615 139816308807424 tf_logging.py:115] TPU job name worker
I0725 16:57:38.520205 139816308807424 tf_logging.py:115] Graph was finalized.
I0725 16:57:41.134776 139816308807424 tf_logging.py:115] Running local_init_op.
I0725 16:57:41.273755 139816308807424 tf_logging.py:115] Done running local_init_op.
I0725 16:57:49.279644 139816308807424 tf_logging.py:115] Saving checkpoints for 0 into gs://fd3add40/model.ckpt.
I0725 16:58:02.819749 139816308807424 tf_logging.py:115] Installing graceful shutdown hook.
2018-07-25 16:58:02.820154: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.
I0725 16:58:02.868602 139816308807424 tf_logging.py:115] Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0', '/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0']
W0725 16:58:02.874579 139816308807424 tf_logging.py:120] Worker heartbeats not supported by all workers.  No failure handling will be enabled.
I0725 16:58:02.874847 139816308807424 tf_logging.py:115] Init TPU system
I0725 16:58:09.173619 139815074658048 tf_logging.py:115] Starting infeed thread controller.
I0725 16:58:09.174196 139815066265344 tf_logging.py:115] Starting outfeed thread controller.
I0725 16:58:09.319966 139816308807424 tf_logging.py:115] Enqueue next (1251) batch(es) of data to infeed.
I0725 16:58:09.320504 139816308807424 tf_logging.py:115] Dequeue next (1251) batch(es) of data from outfeed.
I0725 17:04:17.947448 139816308807424 tf_logging.py:115] Saving checkpoints for 1251 into gs://fd3add40/model.ckpt.
I0725 17:04:27.437869 139816308807424 tf_logging.py:115] loss = 6.3166966, step = 1251
I0725 17:04:27.439600 139816308807424 tf_logging.py:115] Enqueue next (1251) batch(es) of data to infeed.
I0725 17:04:27.439871 139816308807424 tf_logging.py:115] Dequeue next (1251) batch(es) of data from outfeed.
I0725 17:10:17.187642 139816308807424 tf_logging.py:115] Saving checkpoints for 2502 into gs://fd3add40/model.ckpt.
I0725 17:10:27.787137 139816308807424 tf_logging.py:115] loss = 5.30481, step = 2502 (360.349 sec)
I0725 17:10:27.789474 139816308807424 tf_logging.py:115] global_step/sec: 3.47162
I0725 17:10:27.790872 139816308807424 tf_logging.py:115] examples/sec: 3554.94
I0725 17:10:27.792686 139816308807424 tf_logging.py:115] Enqueue next (1251) batch(es) of data to infeed.
I0725 17:10:27.792932 139816308807424 tf_logging.py:115] Dequeue next (1251) batch(es) of data from outfeed.
I0725 17:16:14.326761 139816308807424 tf_logging.py:115] Saving checkpoints for 3753 into gs://fd3add40/model.ckpt.
I0725 17:16:23.874505 139816308807424 tf_logging.py:115] loss = 4.3771253, step = 3753 (356.087 sec)
I0725 17:16:23.876002 139816308807424 tf_logging.py:115] global_step/sec: 3.51319
I0725 17:16:23.876509 139816308807424 tf_logging.py:115] examples/sec: 3597.51
I0725 17:16:23.877604 139816308807424 tf_logging.py:115] Enqueue next (1247) batch(es) of data to infeed.
I0725 17:16:23.877901 139816308807424 tf_logging.py:115] Dequeue next (1247) batch(es) of data from outfeed.
I0725 17:22:10.177654 139816308807424 tf_logging.py:115] loss = 3.9155605, step = 5000 (346.303 sec)
I0725 17:22:10.179430 139816308807424 tf_logging.py:115] global_step/sec: 3.60089
I0725 17:22:10.179932 139816308807424 tf_logging.py:115] examples/sec: 3687.31
I0725 17:22:10.181097 139816308807424 tf_logging.py:115] Saving checkpoints for 5000 into gs://fd3add40/model.ckpt.
I0725 17:22:19.976217 139816308807424 tf_logging.py:115] Stop infeed thread controller
I0725 17:22:19.976502 139816308807424 tf_logging.py:115] Shutting down InfeedController thread.
I0725 17:22:19.976798 139815074658048 tf_logging.py:115] InfeedController received shutdown signal, stopping.
I0725 17:22:19.977138 139815074658048 tf_logging.py:115] Infeed thread finished, shutting down.
I0725 17:22:19.978296 139816308807424 tf_logging.py:115] Stop output thread controller
I0725 17:22:19.978506 139816308807424 tf_logging.py:115] Shutting down OutfeedController thread.
I0725 17:22:19.978672 139815066265344 tf_logging.py:115] OutfeedController received shutdown signal, stopping.
I0725 17:22:19.979084 139815066265344 tf_logging.py:115] Outfeed thread finished, shutting down.
I0725 17:22:19.979393 139816308807424 tf_logging.py:115] Shutdown TPU system.
I0725 17:22:21.458549 139816308807424 tf_logging.py:115] Loss for final step: 3.9155605.
I0725 17:22:21.459045 139816308807424 tf_logging.py:115] Starting to evaluate.
I0725 17:22:21.771024 139816308807424 tf_logging.py:115] Calling model_fn.
I0725 17:22:25.017402 139816308807424 tf_logging.py:115] Done calling model_fn.
I0725 17:22:25.033205 139816308807424 tf_logging.py:115] Starting evaluation at 2018-07-25-17:22:25
I0725 17:22:25.033466 139816308807424 tf_logging.py:115] TPU job name worker
I0725 17:22:25.830353 139816308807424 tf_logging.py:115] Graph was finalized.
I0725 17:22:25.831166 139816308807424 tf_logging.py:115] Restoring parameters from gs://fd3add40/model.ckpt-5000
I0725 17:22:27.618861 139816308807424 tf_logging.py:115] Running local_init_op.
I0725 17:22:27.669819 139816308807424 tf_logging.py:115] Done running local_init_op.
I0725 17:22:27.870759 139816308807424 tf_logging.py:115] Init TPU system
I0725 17:22:42.142838 139816204605184 tf_logging.py:115] Starting infeed thread controller.
I0725 17:22:42.143809 139816221390592 tf_logging.py:115] Starting outfeed thread controller.
I0725 17:22:42.423908 139816308807424 tf_logging.py:115] Enqueue next (48) batch(es) of data to infeed.
I0725 17:22:42.424242 139816308807424 tf_logging.py:115] Dequeue next (48) batch(es) of data from outfeed.
I0725 17:23:03.305603 139816308807424 tf_logging.py:115] Evaluation [48/48]
I0725 17:23:03.305908 139816308807424 tf_logging.py:115] Stop infeed thread controller
I0725 17:23:03.305974 139816308807424 tf_logging.py:115] Shutting down InfeedController thread.
I0725 17:23:03.306147 139816204605184 tf_logging.py:115] InfeedController received shutdown signal, stopping.
I0725 17:23:03.306328 139816204605184 tf_logging.py:115] Infeed thread finished, shutting down.
I0725 17:23:03.306478 139816308807424 tf_logging.py:115] Stop output thread controller
I0725 17:23:03.306602 139816308807424 tf_logging.py:115] Shutting down OutfeedController thread.
I0725 17:23:03.306729 139816221390592 tf_logging.py:115] OutfeedController received shutdown signal, stopping.
I0725 17:23:03.306835 139816221390592 tf_logging.py:115] Outfeed thread finished, shutting down.
I0725 17:23:03.307019 139816308807424 tf_logging.py:115] Shutdown TPU system.
I0725 17:23:03.677274 139816308807424 tf_logging.py:115] Finished evaluation at 2018-07-25-17:23:03
I0725 17:23:03.677593 139816308807424 tf_logging.py:115] Saving dict for global step 5000: global_step = 5000, loss = 3.636791, top_1_accuracy = 0.3595581, top_5_accuracy = 0.63112384
I0725 17:23:06.555644 139816308807424 tf_logging.py:115] Saving 'checkpoint_path' summary for global step 5000: gs://fd3add40/model.ckpt-5000
I0725 17:23:07.066659 139816308807424 tf_logging.py:115] Eval results: {'loss': 3.636791, 'top_1_accuracy': 0.3595581, 'global_step': 5000, 'top_5_accuracy': 0.63112384}
I0725 17:23:08.081572 139816308807424 tf_logging.py:115] Calling model_fn.
I0725 17:23:16.381287 139816308807424 tf_logging.py:115] Create CheckpointSaverHook.
I0725 17:23:16.598444 139816308807424 tf_logging.py:115] Done calling model_fn.
I0725 17:23:16.599975 139816308807424 tf_logging.py:115] TPU job name worker
I0725 17:23:17.820116 139816308807424 tf_logging.py:115] Graph was finalized.
I0725 17:23:17.941310 139816308807424 tf_logging.py:115] Restoring parameters from gs://fd3add40/model.ckpt-5000
I0725 17:23:20.797441 139816308807424 tf_logging.py:115] Running local_init_op.
I0725 17:23:20.966442 139816308807424 tf_logging.py:115] Done running local_init_op.
I0725 17:23:28.111387 139816308807424 tf_logging.py:115] Saving checkpoints for 5000 into gs://fd3add40/model.ckpt.
I0725 17:23:38.181761 139816308807424 tf_logging.py:115] Installing graceful shutdown hook.
2018-07-25 17:23:38.182184: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:349] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.
I0725 17:23:38.184660 139816308807424 tf_logging.py:115] Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0', '/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0']
W0725 17:23:38.189342 139816308807424 tf_logging.py:120] Worker heartbeats not supported by all workers.  No failure handling will be enabled.
I0725 17:23:38.189518 139816308807424 tf_logging.py:115] Init TPU system
I0725 17:23:53.616530 139815066265344 tf_logging.py:115] Starting infeed thread controller.
I0725 17:23:53.617249 139815057872640 tf_logging.py:115] Starting outfeed thread controller.
I0725 17:23:53.775310 139816308807424 tf_logging.py:115] Enqueue next (1251) batch(es) of data to infeed.
I0725 17:23:53.775712 139816308807424 tf_logging.py:115] Dequeue next (1251) batch(es) of data from outfeed.
I0725 17:30:01.452604 139816308807424 tf_logging.py:115] Saving checkpoints for 6251 into gs://fd3add40/model.ckpt.
I0725 17:30:12.238488 139816308807424 tf_logging.py:115] loss = 3.774139, step = 6251
I0725 17:30:12.241529 139816308807424 tf_logging.py:115] Enqueue next (1251) batch(es) of data to infeed.
I0725 17:30:12.241873 139816308807424 tf_logging.py:115] Dequeue next (1251) batch(es) of data from outfeed.
I0725 17:36:02.135157 139816308807424 tf_logging.py:115] Saving checkpoints for 7502 into gs://fd3add40/model.ckpt.
I0725 17:36:11.482980 139816308807424 tf_logging.py:115] loss = 3.2543745, step = 7502 (359.244 sec)
I0725 17:36:11.484921 139816308807424 tf_logging.py:115] global_step/sec: 3.48231
I0725 17:36:11.485519 139816308807424 tf_logging.py:115] examples/sec: 3565.89
I0725 17:36:11.487184 139816308807424 tf_logging.py:115] Enqueue next (1251) batch(es) of data to infeed.
I0725 17:36:11.487564 139816308807424 tf_logging.py:115] Dequeue next (1251) batch(es) of data from outfeed.
I0725 17:41:58.112682 139816308807424 tf_logging.py:115] Saving checkpoints for 8753 into gs://fd3add40/model.ckpt.
I0725 17:42:09.396486 139816308807424 tf_logging.py:115] loss = 3.1598916, step = 8753 (357.914 sec)
I0725 17:42:09.397990 139816308807424 tf_logging.py:115] global_step/sec: 3.49526
I0725 17:42:09.398309 139816308807424 tf_logging.py:115] examples/sec: 3579.15
I0725 17:42:09.399405 139816308807424 tf_logging.py:115] Enqueue next (1247) batch(es) of data to infeed.
I0725 17:42:09.399540 139816308807424 tf_logging.py:115] Dequeue next (1247) batch(es) of data from outfeed.
I0725 17:47:55.500725 139816308807424 tf_logging.py:115] loss = 3.054053, step = 10000 (346.104 sec)
I0725 17:47:55.502393 139816308807424 tf_logging.py:115] global_step/sec: 3.60296
I0725 17:47:55.502839 139816308807424 tf_logging.py:115] examples/sec: 3689.43
I0725 17:47:55.504043 139816308807424 tf_logging.py:115] Saving checkpoints for 10000 into gs://fd3add40/model.ckpt.
I0725 17:48:05.418097 139816308807424 tf_logging.py:115] Stop infeed thread controller
I0725 17:48:05.418373 139816308807424 tf_logging.py:115] Shutting down InfeedController thread.
I0725 17:48:05.418592 139815066265344 tf_logging.py:115] InfeedController received shutdown signal, stopping.
I0725 17:48:05.418761 139815066265344 tf_logging.py:115] Infeed thread finished, shutting down.
I0725 17:48:05.418992 139816308807424 tf_logging.py:115] Stop output thread controller
I0725 17:48:05.419121 139816308807424 tf_logging.py:115] Shutting down OutfeedController thread.
I0725 17:48:05.419236 139815057872640 tf_logging.py:115] OutfeedController received shutdown signal, stopping.
I0725 17:48:05.419351 139815057872640 tf_logging.py:115] Outfeed thread finished, shutting down.
I0725 17:48:05.419496 139816308807424 tf_logging.py:115] Shutdown TPU system.
I0725 17:48:07.101744 139816308807424 tf_logging.py:115] Loss for final step: 3.054053.
I0725 17:48:07.102034 139816308807424 tf_logging.py:115] Starting to evaluate.
I0725 17:48:07.423268 139816308807424 tf_logging.py:115] Calling model_fn.
I0725 17:48:10.714258 139816308807424 tf_logging.py:115] Done calling model_fn.
I0725 17:48:10.730081 139816308807424 tf_logging.py:115] Starting evaluation at 2018-07-25-17:48:10
I0725 17:48:10.730343 139816308807424 tf_logging.py:115] TPU job name worker
I0725 17:48:11.407757 139816308807424 tf_logging.py:115] Graph was finalized.
I0725 17:48:11.408488 139816308807424 tf_logging.py:115] Restoring parameters from gs://fd3add40/model.ckpt-10000
I0725 17:48:13.147605 139816308807424 tf_logging.py:115] Running local_init_op.
I0725 17:48:13.202266 139816308807424 tf_logging.py:115] Done running local_init_op.
I0725 17:48:13.392245 139816308807424 tf_logging.py:115] Init TPU system
I0725 17:48:24.514977 139815345452800 tf_logging.py:115] Starting infeed thread controller.
I0725 17:48:24.515423 139815066265344 tf_logging.py:115] Starting outfeed thread controller.
I0725 17:48:24.834467 139816308807424 tf_logging.py:115] Enqueue next (48) batch(es) of data to infeed.
I0725 17:48:24.834784 139816308807424 tf_logging.py:115] Dequeue next (48) batch(es) of data from outfeed.
I0725 17:48:46.116372 139816308807424 tf_logging.py:115] Evaluation [48/48]
I0725 17:48:46.116667 139816308807424 tf_logging.py:115] Stop infeed thread controller
I0725 17:48:46.116733 139816308807424 tf_logging.py:115] Shutting down InfeedController thread.
I0725 17:48:46.116918 139815345452800 tf_logging.py:115] InfeedController received shutdown signal, stopping.
I0725 17:48:46.117114 139815345452800 tf_logging.py:115] Infeed thread finished, shutting down.
I0725 17:48:46.117234 139816308807424 tf_logging.py:115] Stop output thread controller
I0725 17:48:46.117322 139816308807424 tf_logging.py:115] Shutting down OutfeedController thread.
I0725 17:48:46.117448 139815066265344 tf_logging.py:115] OutfeedController received shutdown signal, stopping.
I0725 17:48:46.117584 139815066265344 tf_logging.py:115] Outfeed thread finished, shutting down.
I0725 17:48:46.117705 139816308807424 tf_logging.py:115] Shutdown TPU system.
I0725 17:48:46.495675 139816308807424 tf_logging.py:115] Finished evaluation at 2018-07-25-17:48:46
I0725 17:48:46.496052 139816308807424 tf_logging.py:115] Saving dict for global step 10000: global_step = 10000, loss = 3.0327156, top_1_accuracy = 0.4957479, top_5_accuracy = 0.75476074
I0725 17:48:47.081310 139816308807424 tf_logging.py:115] Saving 'checkpoint_path' summary for global step 10000: gs://fd3add40/model.ckpt-10000
I0725 17:48:47.515832 139816308807424 tf_logging.py:115] Eval results: {'loss': 3.0327156, 'top_1_accuracy': 0.4957479, 'global_step': 10000, 'top_5_accuracy': 0.75476074}
I0726 02:38:55.471451 139816308807424 tf_logging.py:115] Finished training up to step 112590. Elapsed seconds 34890.
